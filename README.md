# Stanford-Question-Answering-SQuAD-System
In this project work we build two Extractive Question Answering systems (Span based) on the SQuAD dataset. Our models are fine-tuned versions of the pretrained TFBertModel (uncased version) and differ by the number of epochs they were fine-tuned for and the max len hyperparameter which allows to specify the maximum length of the input sequences that are passed to the models. The available dataset is split into train and validation subsets using a train-val split ratio (80:20). After the fine tuning phase the models were saved and loaded for further use, specifically to evaluate their performances on the validation set (used as a test set). We relied on a custom metric, the Exact Match metric, implementend as a Keras callback function to be invoked at the end of every training (or fine-tuning) epoch. To further evaluate the models at test time the provided evaluate.py Python script was used. To summarise, BERT512, the model where max len is equal to 512, achieved better performances than BERT384 even though the latter was fine-tuned for a double amount of epochs(6 instead of 3).
